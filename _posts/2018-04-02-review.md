---
layout: post
title: 面试经历
categories: [面试]
description: 面试经历
keywords: 面试经历
---


<h2 align = "center"> 面试经历 </h2>

<br/>

## 1. 2018-04-02  今日头条 商业变现算法实习生

  找的内推，听说是头条的核心部门，唯一的赚钱的来源。
  不知道具体有几面，面了第一轮就凉凉了。感觉面试还是有点难度的，就是面试官喜欢从你的答案里面，一直去找新的问题，一直深挖。比较考细节和基础。

### 自我介绍

  介绍自己的学校、方向，然后说了一下我的方向是具体做什么的，他问了一些比较简单的问题，比如你们经常用什么模型之类的，没有涉及细节。

### 提问

#### ** 介绍一下你觉得做的比较好的项目？ **
  我说了我在实验室里面做的辐射源识别的东西，跟他解释了背景，数据的特点，常用的方法。

#### ** 你们都常用哪些分类的模型？ **
  SVM, NN, RF, XGBOOST、CNN等

#### ** 你具体说一下你们怎么用CNN的？ **
  我介绍了我的论文，说设计了一种编码的算法来处理特征不等长，然后用CNN分类

#### ** (然后给了纸) 说一下你的数据输入有什么特点？ **
  我详细解释了一下

#### ** 你画一下你们一维CNN的结构，解释一下每一层的结构？ **
  提了很多问题，例如：为什么要设置这么多的输入节点？为什么要设置3层卷积？不同类别特征怎么组合的？

#### ** DNN模型中，会出现梯度消失，解释一下为什么？ **

  我回答sigmoid，tahn的特点造成的，具体说了一下函数的梯度的特点。

#### ** (追问) 说一下梯度消失有哪些解决方法？为什么可以解决？ **

  我回答RELU和Batch Normalization，RELU的分段线性，BN对每一层输入进行中心化归一化，都可以保证梯度的大小在一定的范围内。 期间画了RULU,tanh,sigmoid的曲线，BN之前和之后的效果。

#### ** 那模型训练如果出现过拟合，你怎么处理？ **

  我回答：1.增加数据：采集更多数据、数据增强；
         2.降低模型复杂度: 提前结束训练、选择更简单的结构
         3.正则化操作：L1/L2正则化
         4.针对不同的模型：DNN: dropout, 树结构:剪枝

#### ** 解释一下L1/L2正则化的区别？ **
  我回答：L1权重的一范数，l2是2范数。L1正则化会导致很多权重为0，L2则相对平稳分布在0附近。原理:L1之后参数服从拉普拉斯分布，L2服从正太分布。

#### ** CNN里面一般用什么损失函数？ **

  我一时没想起来，我先回答了平方和。。。。(乱答的)
  他说：？？？然后瞬间想起来，tf里面用的是交叉熵。然后答上了。。。

#### ** 写一下交叉熵的公式？ **

  我想了一下，然后写出来了，没毛病。

#### ** 解释一下为什么交叉熵work？**
  然后我一脸懵逼，画了个图，强行解释了一波。他说不对，我说那我不会。然后他就给解释了一下，虽然我没听懂。。。


### 算法题

  他说我们来做点题，问：你对你的数学自信一些，还是对编程自信一点？
  答：编程吧.....(这么一问，我心一下就凉了...)

#### ** 1、问了跳台阶的问题，n级台阶，每次可以跳1,2,3步，问共有多少种到n的跳法？ **

  答：这个题有了解过，是动态规划，有递推公式
  问：你写一下递推公式
  写的没有问题。
  问：那看来你这个问题掌握的不错，那怎么来点难一点的。（哭）

#### ** 2、如果现在有n级台阶，每次可以跳1,2,...m步，算一下跳到n有多少种跳法？ **

  问：你想一想，给我说一下思路？

  答：好像也有递推公式，我写了一个，考虑m > n, m < n，可以用递推实现。（突然想起剑指offer里面，不要用递归...）

  问：那你写一下代码实现，但是你的解法还有很多可以优化的，你是先优化一下再写，还是先写？

  答：先写吧(我怕我想不出来，就说先写，熟悉一下这个问题)

  磕磕绊绊写出了代码，花了好几分钟。

  问：你觉得有什么可以优化的？

  答：(已经意识到不能用递归了) 我觉得递归效率太低了，我改成循环吧。然后改了代码。

  问：你说一下时间空间复杂度？

  答：时间:O(nm),空间O(n)

  问：那如果你的n是100w呢，你觉得有什么可以改进的？

  答：空间复杂度可以降，没有必要存前面每一步的结果，存前m步就可以了。空间复杂度可以降到O(m)

  问：你考虑一下时间复杂度怎么降？
  答：（思考了一下）求和的过程中，有很多重复求的过程，可以用一个滑动窗口，每次减去最开头的，加上最后的。

  问：那你写一下递推公式？

  答：在纸上写了出来。

  问：你觉得还有什么地方可以改进的？

  答：我觉得已经很好的...

  问：现在的时间空间复杂度？

  答：时间O(n),空间O(m)

  问：你求前m步的时候，时间复杂度呢？

  答：O(m2)

  问：怎么优化？

  答：还是用滑动窗口

  问：是的，现在就最优结果了，你写一下通项公式。

  答：写在了纸上

  问：化简一下n<m的结果

  答：很快改了

  然后就写出来了。。。真是艰难。。。

  最后就是这么一个公式：

  f(n, m) = 2 * f(n-1, m) (n < m)

  f(n, m) = 2 * f(n-1, m) - f(n-1-m, m) (n >= m)

  面试结束，持续一个半小时


面试官说出去一会，让等他一下。几分钟之后回来，说，不好意思，和面试的岗位不匹配。

凉凉，我的第一场面试。

总的来说，问的问题不太难，大部分都能答上来，但是后面编程题做的不尽如人意。


## 2. 2018-04-03  搜狗 信息流推荐算法实习生

3月20号前后就在系统里面投了实习，但是一直没有人处理简历。。。于是3月31号，我直接把简历投到了HR的邮箱里面，说了自己想去的岗位（网页搜索部门的【数据挖掘研究员】，精准/信息流广告部门的【信息流广告策略研究员】）。

然后4月1号，搜狗那边就打电话让去面试一下，效率还挺高，但是我忘了问具体的岗位是啥。

公司在清华科技园，离我们宿舍和所里的距离，应该都只有几百米，其实我还挺乐意去的。（头条也离的近，也挺想去的，只可惜跪了）

### 一面

上来之后没有让自我介绍，负责人直接说他们是干什么的，做的产品就是搜狗搜索首页的信息流推荐，和今日头条差不多。

然后开始挨个讲项目。中间我巴拉巴拉各种解释问题的背景，我的解决方案。


#### **常用什么模型**

答：LR, SVM, RF, XGBOOST, CNN等

#### **LR和SVM，你一般怎么去选？**

答：看数据量和特征的量，一般特征少，样本少，我会用SVM；
如果特征和样本量都比较多，我会用LR来做；
如果样本少，特征多，线性的SVM，或者做一下特征筛选；
如果特征少，样本多，考虑LR，或者带kernel的SVM

#### **你讲一下XGBOOST的原理？**

答：一种梯度提升的方法，boosting的一种实现，它是一种加法模型，使用前向分步算法来求解。它是在经典的GBDT的基础上的改进。（其实说的很笼统，不知道怎么去说清楚）

#### **那你说一下GDBT的XGBOOST的区别**

答：（1）XGBOOST引入了正则项，限制叶子节点和权重；

（2）XGBOOST可以自定义损失函数；

（3）XGBOOST使用牛顿法来进行残差估计，GDBT使用梯度法，前者是一阶泰勒展开，后者是二阶泰勒展开；

然后我就记不清了，实际上还有XGBOOST的并行化、剪枝...不过最重要的正则化还有残差估计答上来了。

#### **TensorFlow用到什么程度，你说一个你觉得在项目中用的比较好的api？**

答：Tensorflow之前看书学过，但是用的不是特别熟。。。（我对tf真是用的不熟）然后我说，我觉得用的比较高级的功能，可能就是可视化和fine-tuning了。

这个答得很烂。

#### **你对推荐算法了解多少？知道FM吗？**

答：之前上课有学过一些简单的推荐算法，比如协同过滤；

FM不好意思真的布吉岛。。。（没听过）

#### **看你做特征做的挺多的，你讲一下你做特征工程的流程？**

答：（1）深刻理解数据的背景知识；

（2）多角度全面设计特征；

（3）提特征前做数据清洗：然后讲了噪声的处理方法，缺失值的处理方法。



然后详细聊了各个项目的情况，讲了很久。问了实习的时间。跟我说了产品的情况。


### 二面

Boss来了，

#### 继续讲一个项目，问用到什么方法

#### LR的损失函数写一下？

#### XGBOOST相对GBDT的改进

#### 做题
